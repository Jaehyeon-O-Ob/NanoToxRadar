{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep learning models which were used in this research\n",
    "\n",
    "1. MLP (Multi-Layer Perceptron - Default)\n",
    "\n",
    "2. Transformer(Encoder block - Description model)\n",
    "\n",
    "Created by Jaehyeon Park\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tox_mlp(nn.Module):\n",
    "    def __init__(self, input_size, dropout):\n",
    "        super(tox_mlp, self).__init__()\n",
    "        self.total_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.BatchNorm1d(input_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.total_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tox_mlp(\n",
      "  (total_layer): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): GELU(approximate='none')\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): GELU(approximate='none')\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): GELU(approximate='none')\n",
      "    (15): Dropout(p=0.2, inplace=False)\n",
      "    (16): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (17): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): GELU(approximate='none')\n",
      "    (19): Dropout(p=0.2, inplace=False)\n",
      "    (20): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp_model = tox_mlp(input_size=512, dropout=0.2)\n",
    "\n",
    "print(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tox_transformer(nn.Module):\n",
    "    def __init__(self, feature_dim, d_model, nhead, num_layers, dropout):\n",
    "        super(tox_transformer, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(1, 2)\n",
    "\n",
    "        self.dim_transform = nn.Linear(feature_dim, d_model)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\"            \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, \n",
    "            num_layers=num_layers,\n",
    "            enable_nested_tensor=False)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.output_linear = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # [B, Fd, 1]\n",
    "        x = self.linear(x)   # [B, Fd, 2]\n",
    "        x = x.permute(0, 2, 1)  # [B, 2, Fd]\n",
    "        \n",
    "        x = self.dim_transform(x)  # [B, 2, d_model]\n",
    "        \n",
    "        x = self.transformer_encoder(x)  # [B, 2, d_model]\n",
    "        x = x.permute(0, 2, 1)  # [B, d_model, 2]\n",
    "        x = self.avg_pool(x)    # [B, d_model, 1]\n",
    "        x = x.squeeze(-1)       # [B, d_model]\n",
    "        x = self.output_linear(x)  # [B, 1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tox_transformer(\n",
      "  (linear): Linear(in_features=1, out_features=2, bias=True)\n",
      "  (dim_transform): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (output_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformer_model = tox_transformer(feature_dim=512, \n",
    "                                    d_model=256, \n",
    "                                    nhead=2, \n",
    "                                    num_layers=2,\n",
    "                                    dropout=0.2)\n",
    "\n",
    "print(transformer_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
